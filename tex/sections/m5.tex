Up until now we have been implementing our operating system on the assumption that it runs on a single core. With the introduction of message passing in the last Milestone, we are at the point where we can start harnessing the full power of the underlying computer's CPU by using all of its available cores. In this Milestone we implement multicore system support for our OS and begin thinking about cross-core communication.
\\\\
Barrelfish is a \textit{multikernel}. Instead of running all operating system services in a single address space (a ``monolithic" kernel), it treats multicore machines as a distributed system. That is, each core on the system runs its own kernel in a separate address spaces. Instead of assuming shared memory for inter-process communication (IPC), we rely primarily on message-passing (similar to LMP) for synchronization and distribution of workloads. In this case, system invariants are enforced not through locks, but through distributed algorithms.
\\\\
Our goals in this Milestone didn't require too much thought about design. We were interested mainly in successfully booting a second core, giving it a subset of the physical address space, and getting some sort of primitive communication going between it and the first core. We provide a walkthrough of how each of these things were achieved.

\subsection{Booting a New Core}
The first thing to do is allocate memory for the new core. We use our memory manager to allocate RAM for the various bits and pieces necessary, and then store their base addresses and sizes in the ``Coredata" structure. Coredata stores boot paramaters for the new core such as its ID, how much memory it has, and as we just mentioned, where to find the memory it needs for initial execution. Note that our bootdriver and CPU driver use the kernel page table, which implements a 1-to-1 mapping between physical and virtual address space (in other words, we're using physical addresses). Also note that we must \textit{relocate} the bootdriver and CPU driver since they are linked to run at a specified address, but in reality run at another. The result is that global variables now have differing addresses and thus need to be relocated. The new core needs the following:
\begin{itemize}[itemsep=0pt]
    \item The ``kernel conrol block" (KCB), which stores the state of the kernel, requires memory. We need to provide an empty state for the new core to start with, after which it will handle this on its own. We allocate RAM, retype the RAM to a KCB capability, and then store its physical address in the Coredata.
    \item \texttt{init}, since this will be the first process to run on the new core once it boots. We load the binary and store its size/physical address in the Coredata.
    \item The bootdriver/CPU driver, which start up the core when we tell it to begin (the CPU driver keeps running after boot as well). The CPU driver is allocated, loaded, and relocated, with its entrypoint being stored in the Coredata. The bootdriver is also allocated, loaded, and relocated, however it is instead handed back to the caller since it is run using multiboot, which are just binary images built into the OS to allow us to run applications without a full-fledged filesystem.
    \item At least 16 pages of stack memory for the CPU driver, necessary for its execution. We allocate this and store the size/physical address in the Coredata.
    \item A small amount (roughly 5MB) of memory to get the core running. Remember that it cannot manage its own memory during startup, since \texttt{init} has not even begun running. The size and address of the memory is stored in the Coredata.
    \item A URPC frame, necessary for cross-core communication. We explain this in-depth at a later time. Its size and address are stored in the Coredata.
    \item The location of the multiboot strings. We need this particularly to spawn processes from the multiboot images. Its address is stored in the Coredata
    \item The physical address to memory holding Boot information structure, which has the physical addresses of starting RAM regions for \texttt{init} to add to its memory manager. This structure also includes addresses to the ELF images from multiboot modules, which is necessary for spawning processes without a functioning filesystem.
\end{itemize}
Once we have all the information necessary for the new core's Coredata, we allocate it, fill it in, and get its physical address, which will subsequently be handed off to \texttt{init}. Before the call to \texttt{init} however, we need to ensure that the data we've written is visible to the core, since its MMU will not yet be on. This requires a data-memory barrier (guaranteeing that all loads and stores up to this point in execution have completed) and a cache invalidation/clean (to ensure that all information is fresh and actually in memory). Finally, we may invoke the monitor (\texttt{init}) to spawn our new core.

\subsubsection{Memory Allocation on New Cores}
Remember that in a multikernel, each core is running its own instance of the kernel with its own part of memory. We now have the core booting part done, but we still need to figure out how to split and hand off memory to each core. We implemented a rather simple technique to achieve this, just to start off. The approach is basically to manually split memory for each core; since we were only concerned with running two cores to start, the first core checks if it is, in fact, the first core. If this is the case, its (the first core's) memory manager takes half of the available physical memory and marks it as in use. On boot, the second core confirms that it is not the first core, and then ``forges" capabilities for the available remaining, unmarked RAM (the other half). That RAM is then handed to its (the second core's) memory manager. The operation of forging RAM invokes the kernel capability to create a new capability for a given region of physical memory. This ``forging" must be done, since each core runs its own CPU driver and monitor---there is currently no concept of passing capabilities between cores (this is updated in the Capabilities portion of Milestone 7).
\\\\
Note that a slightly more clever scheme could be used here to divide the address space based on an arbitrary amount supported cores, but this would be trivial (in theory...) and we were only concerned with the two-core case to begin. Also note that we \textit{must} start off new cores with some amount of physical memory since they need to actually be able to run a few operations in \texttt{init} to set up communication channels to request more memory dynamically. After this happens however, processes on cores may begin to exchange RAM through IPC.

\subsubsection{Spawning Processes on New Cores}
In order to spawn processes on our new core, it needs to have access to the multiboot modules, which contain address to their ELF images in memory. This is quite straightforward since we have all of the pieces necessary to do this already. The ``boot info" structure handed to the new core contains an array of memory regions; some may be marked as multiboot modules, so we do a search for them, For any found modules, we invoke the kernel capability to forge a frame capability pointing at the module, and we're ready to go. Since the new core has the module in memory, its location, and a capability for it, we can now spawn the associated process just as was done in Milestone 3.

\subsection{Cross-Core Communication}
This part of the Milestone serves as an introduction to communicating between cores; Milestone 6 will explore this in depth. Remember yet again that we're building a multikernel, which means that each core works as a node in a network running its own instance of the kernel. Cross-core communication therefore is vital. Before implementing a full cross-core IPC system in Milestone 6, we start by implementing a simple shared-memory communication channel between our two cores.
\\\\
When booting new cores, a ``user-level RPC" (URPC) frame gets allocated. The URPC frame is a piece of shared memory between cores that we can use to communicate over, using ``UMP" (user-level message passing). After the second core boots, a UMP frame is mapped, initialized, and its channels are opened. In order to actually communicate over the frame, we use a ring buffer. Sending messages involves writing our data to the buffer and then setting a ``control word" at the end when the data is ready to be received. Receiving messages involves polling on that control word and then reading the associated data when it is ready. When a message is confirmed as received, both sides of the channel can advance their pointers in the ring buffer. Note that in contrast to LMP, UMP supports full-duplex connections using \textit{two} channels; one for sending, and one for receiving.
\\\\
During Milestone 5, we did not start with a rudimentary communication protocol, but started to implement UMP in its entirety as we felt that it would be more efficient to do so. Thus, we reserve the details for this communication protocol, as it is the primary topic of the following section. 
